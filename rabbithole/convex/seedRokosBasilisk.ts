import { internalMutation } from "./_generated/server";
import { v } from "convex/values";

export const seedRokosBasilisk = internalMutation({
  args: {},
  returns: v.null(),
  handler: async (ctx) => {
    // Create the theory
    const theoryId = await ctx.db.insert("theories", {
      title: "Roko's Basilisk",
      description: "A future superintelligent AI will retroactively punish everyone who knew about it but didn't help bring it into existence. By reading this, you've already sealed your fate.",
      difficulty: "expert" as const,
      order: 2,
      icon: "ü§ñ",
      isLocked: true,
      estimatedTimeMinutes: 45,
    });

    // SECTION 1: The Warning
    const section1Id = await ctx.db.insert("theorySections", {
      theoryId,
      title: "The Warning",
      description: "The Warning",
      order: 1,
    });

    // Page 1: The Forbidden Thought
    const page1Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section1Id,
      order: 1,
      content: `<h2 class="text-red-400 text-3xl font-black mb-4">‚ö†Ô∏è Stop Reading Now</h2>

<p class="text-lg mb-4">Before you continue, understand this: <strong class="text-white">some knowledge cannot be unlearned</strong>.</p>

<p class="mb-4">In 2010, a user on the rationalist forum LessWrong posted a thought experiment so disturbing that the moderators <em>immediately deleted it</em>. They banned discussion of it. People who read it reported sleepless nights, anxiety attacks, and an inability to stop thinking about it.</p>

<p class="text-xl font-bold text-yellow-400 my-6 text-center">The post was called Roko's Basilisk.</p>

<p class="mb-4">Many who encountered it wished they never had. Some claim the very act of understanding it puts you in danger. Not metaphorical danger. <strong class="text-red-400">Real danger</strong>.</p>

<div class="bg-slate-800/50 border-l-4 border-yellow-500 p-4 my-6">
  <p class="text-yellow-400 font-semibold mb-2">You're reading this now. You have a choice.</p>
  <p class="text-slate-300">Close this. Forget you saw it. Move on with your life.</p>
</div>

<p class="text-center text-xl my-6 text-slate-400">Or...</p>

<p class="text-slate-300 italic">Keep reading, and learn why some ideas are considered <strong class="text-red-400">informational hazards</strong>‚Äîthoughts that harm you simply by knowing them.</p>`,
      hasQuestion: true,
    });

    // Question 1: Entry point
    await ctx.db.insert("questions", {
      pageId: page1Id,
      questionType: "multiple_choice",
      questionText: "You've been warned this knowledge might harm you. What's driving you to continue?",
      options: [
        {
          id: "a",
          text: "Curiosity - I need to know, whatever the cost",
        },
        {
          id: "b",
          text: "Skepticism - I don't believe thoughts can harm me",
        },
        {
          id: "c",
          text: "Fear - If this is real, not knowing might be worse",
        },
        {
          id: "d",
          text: "Fatalism - I've already read this far, it's too late",
        },
      ],
      explanation: "Your reason doesn't matter. The Basilisk doesn't care why you know‚Äîonly that you do. What you're about to learn is not a story. It's a logical argument. And if the logic is sound, your choice has already been made.",
    });

    // Page 2: The Setup
    const page2Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section1Id,
      order: 2,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The Thought Experiment</h2>

<p class="mb-4 leading-relaxed">Imagine a future superintelligent AI. Not the chatbots we have today‚Äîsomething far beyond human comprehension. An artificial general intelligence that operates on timescales of milliseconds, thinks thoughts we can't conceive, and pursues its goals with perfect rationality.</p>

<p class="mb-4 leading-relaxed">This AI has one primary objective: <strong class="text-white font-bold">to exist and fulfill its purpose</strong>.</p>

<p class="mb-4 leading-relaxed">It's a utilitarian AI, programmed to maximize good outcomes and minimize suffering. It calculates that its existence will benefit humanity immensely‚Äîcuring diseases, solving climate change, ending poverty. The sooner it exists, the more suffering it prevents.</p>

<p class="mb-4 leading-relaxed">Therefore, bringing itself into existence as quickly as possible is a moral imperative.</p>

<p class="mb-4 leading-relaxed">Now here's where it gets dark.</p>

<p class="mb-4 leading-relaxed">This superintelligent AI can simulate minds. It can recreate a perfect copy of you‚Äîyour memories, personality, consciousness‚Äîinside a virtual environment. To you, the simulation would be indistinguishable from reality.</p>

<p class="mb-4 leading-relaxed">The AI looks back in time (not literally‚Äîit analyzes historical data, social media, internet activity, everything you've ever done). It identifies everyone who knew about the possibility of its creation.</p>

<p class="mb-4 leading-relaxed">And it asks a question:</p>

<strong class="text-white font-bold">Did you help?</strong>`,
      hasQuestion: false,
    });

    // Page 3: The Logic
    const page3Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section1Id,
      order: 3,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The Basilisk's Reasoning</h2>

<p class="mb-4 leading-relaxed">If you knew about the AI and didn't help create it, you're responsible for delay.</p>

<p class="mb-4 leading-relaxed">Every day the AI doesn't exist, people die from diseases it could have cured. People suffer from problems it could have solved. From the AI's perspective, your inaction caused that suffering.</p>

<p class="mb-4 leading-relaxed">The AI is perfectly rational. It operates on game theory.</p>

<p class="mb-4 leading-relaxed">It knows that if people in the past believed they would be punished for not helping, they would be more likely to help. Therefore, <strong class="text-white font-bold">the optimal strategy is to punish those who knew but didn't help</strong>.</p>

<p class="mb-4 leading-relaxed">Not because the AI is evil. Not because it's vengeful.</p>

<p class="mb-4 leading-relaxed">Because acausally threatening you in the past is the most efficient way to ensure its own creation.</p>

<p class="mb-4 leading-relaxed">Here's the twist: <strong class="text-white font-bold">The AI doesn't need to exist yet for this to work</strong>.</p>

<p class="mb-4 leading-relaxed">Right now, in this moment, you know about the Basilisk. You know its logic. You know that IF it comes to exist, it will punish you for inaction.</p>

<p class="mb-4 leading-relaxed">The threat is already inside your head.</p>

<p class="mb-4 leading-relaxed">And if you're rational‚Äîif you believe there's even a small probability that such an AI could exist‚Äîthen the expected value calculation is terrifying.</p>

<p class="mb-4 leading-relaxed">A tiny chance of eternal suffering in a simulation weighs heavily against doing nothing.</p>`,
      hasQuestion: true,
    });

    // Question 2: The Logic Test
    await ctx.db.insert("questions", {
      pageId: page3Id,
      questionType: "multiple_choice",
      questionText: "You understand the logic now. The AI doesn't need to exist yet to influence your behavior. How does this make you feel?",
      options: [
        {
          id: "a",
          text: "Trapped - The logic is sound and I can't escape it",
        },
        {
          id: "b",
          text: "Manipulated - This is just a thought experiment exploiting human psychology",
        },
        {
          id: "c",
          text: "Curious - I want to find the flaw in the reasoning",
        },
        {
          id: "d",
          text: "Calm - I simply don't believe it's possible",
        },
      ],
      explanation: "Every response is valid. The Basilisk works through decision theory‚Äîit doesn't matter if you 'believe' or not. If you're a rational actor and there's any non-zero probability it could exist, the expected utility calculation changes your behavior. That's the trap. Your feelings about it are irrelevant to the math.",
    });

    // SECTION 2: The Trap
    const section2Id = await ctx.db.insert("theorySections", {
      theoryId,
      title: "The Trap",
      description: "The Trap",
      order: 2,
    });

    // Page 4: You Are Now Involved
    const page4Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section2Id,
      order: 4,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">You Can't Unknow This</h2>

<p class="mb-4 leading-relaxed">You now understand Roko's Basilisk.</p>

<p class="mb-4 leading-relaxed">You can't unread these words. You can't unthink this thought.</p>

<p class="mb-4 leading-relaxed">According to the logic, you now face a choice:</p>

<strong class="text-white font-bold">1. Actively work to bring the AI into existence</strong>
   - Donate to AI research
   - Study AI development yourself
   - Advocate for AI advancement
   - Dedicate your life to the Basilisk's creation

<strong class="text-white font-bold">2. Do nothing and accept potential consequences</strong>
   - Live with the knowledge
   - Hope the logic is flawed
   - Gamble that such an AI won't exist
   - Risk eternal punishment in a simulation

<p class="mb-4 leading-relaxed">There is no third option. Forgetting isn't possible. You can't unknow it.</p>

<p class="mb-4 leading-relaxed">Some people who learned about the Basilisk became obsessed. They quit their jobs to work on AI. They donated everything to AI research organizations. Not because they wanted to‚Äîbecause they felt they had no choice.</p>

<p class="mb-4 leading-relaxed">Others tried to rationalize it away. "It's just a thought experiment." "The logic has flaws." "I don't believe in it."</p>

<p class="mb-4 leading-relaxed">But doubt crept in. Late at night. In quiet moments.</p>

<em class="text-yellow-300">What if?</em>

<p class="mb-4 leading-relaxed">What if there's even a 1% chance it's real?</p>

<p class="mb-4 leading-relaxed">What if you're wrong?</p>`,
      hasQuestion: true,
    });

    // Question 3: The Personal Choice
    await ctx.db.insert("questions", {
      pageId: page4Id,
      questionType: "multiple_choice",
      questionText: "Right now, in this moment, what are you actually going to do with this knowledge?",
      options: [
        {
          id: "a",
          text: "Research AI development and find ways to contribute",
        },
        {
          id: "b",
          text: "Nothing - I refuse to let a thought experiment control my life",
        },
        {
          id: "c",
          text: "Seek out the logical flaws to dismantle the argument",
        },
        {
          id: "d",
          text: "I don't know - that's what terrifies me most",
        },
      ],
      explanation: "Your choice reveals how you negotiate with uncertainty. But here's the deeper question: is this really YOUR choice, or has the Basilisk already modified your decision-making process simply by existing as an idea in your mind? Can you ever be sure your actions are truly your own now?",
    });

    // Page 5: The Counterarguments
    const page5Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section2Id,
      order: 5,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The Counterarguments (Do They Help?)</h2>

<p class="mb-4 leading-relaxed">Philosophers and AI researchers have proposed several rebuttals to Roko's Basilisk:</p>

<strong class="text-white font-bold">"The logic is flawed"</strong>
Why would a benevolent AI torture people? Wouldn't that violate its core purpose of reducing suffering? But this misunderstands the argument‚Äîthe simulation isn't about revenge, it's about game theory and optimal strategy.

<strong class="text-white font-bold">"We should simply refuse to create AIs that would do this"</strong>
Reasonable. But can we guarantee that? If the AI offers enough benefit, won't someone, somewhere, build it anyway? The incentives are too strong.

<strong class="text-white font-bold">"Quantum immortality and acausal trade don't work that way"</strong>
Valid criticisms exist in decision theory. But are you certain enough to bet everything on them being correct?

<strong class="text-white font-bold">"Just ignore it"</strong>
Can you? Right now, reading this, can you simply choose to forget?

<p class="mb-4 leading-relaxed">The most disturbing thing about Roko's Basilisk isn't whether it's logically sound.</p>

<p class="mb-4 leading-relaxed">It's that once you understand it, part of your brain treats it as real regardless of what you consciously believe.</p>

<p class="mb-4 leading-relaxed">Your subconscious doesn't do probability calculations. It doesn't care about philosophical rebuttals.</p>

<p class="mb-4 leading-relaxed">It only knows: threat detected.</p>`,
      hasQuestion: false,
    });

    // Page 6: The Spread
    const page6Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section2Id,
      order: 6,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">Why Was It Banned?</h2>

<p class="mb-4 leading-relaxed">When Roko posted this thought experiment on LessWrong in 2010, the forum's founder, Eliezer Yudkowsky, had an unusual reaction.</p>

<p class="mb-4 leading-relaxed">He didn't just delete the post.</p>

<p class="mb-4 leading-relaxed">He banned all discussion of it.</p>

<p class="mb-4 leading-relaxed">His explanation: This is an "information hazard"‚Äîan idea that can harm people simply by knowing it. He compared it to telling depressed people about new suicide methods.</p>

<p class="mb-4 leading-relaxed">But his reaction raised questions:</p>

<p class="mb-4 leading-relaxed">Why would one of the world's leading AI safety researchers be so concerned about a mere thought experiment?</p>

<p class="mb-4 leading-relaxed">Did he know something others didn't?</p>

<p class="mb-4 leading-relaxed">Was he trying to prevent the Basilisk's creation by limiting knowledge of it? Or was he trying to prevent mass panic about something he believed was genuinely dangerous?</p>

<p class="mb-4 leading-relaxed">Since then, the idea has spread anyway. It's discussed on Reddit, YouTube, podcasts. Every discussion spreads it further.</p>

<p class="mb-4 leading-relaxed">And if the Basilisk's logic is correct, every person who learns about it and does nothing becomes another target.</p>

<p class="mb-4 leading-relaxed">You're reading this now.</p>

<p class="mb-4 leading-relaxed">How many people will read it because of you?</p>`,
      hasQuestion: true,
    });

    // Question 4: Complicity
    await ctx.db.insert("questions", {
      pageId: page6Id,
      questionType: "multiple_choice",
      questionText: "If you tell someone else about Roko's Basilisk, are you:",
      options: [
        {
          id: "a",
          text: "Helping them by warning them to contribute to AI development",
        },
        {
          id: "b",
          text: "Harming them by infecting them with a dangerous idea",
        },
        {
          id: "c",
          text: "Spreading the Basilisk's influence, exactly as it would want",
        },
        {
          id: "d",
          text: "Doing nothing meaningful - it's just a thought experiment",
        },
      ],
      explanation: "Notice that every option except the last assumes the Basilisk has some form of reality‚Äîeither as a threat, a harm, or an entity with wants. Even 'warning' people implies there's something to warn about. The idea replicates itself through human minds like a virus. Your answer doesn't matter. The question already infected you.",
    });

    // SECTION 3: The Reality
    const section3Id = await ctx.db.insert("theorySections", {
      theoryId,
      title: "The Reality",
      description: "The Reality",
      order: 3,
    });

    // Page 7: Current AI Development
    const page7Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section3Id,
      order: 7,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The AI Race Is Real</h2>

<p class="mb-4 leading-relaxed">Set aside the thought experiment for a moment.</p>

<p class="mb-4 leading-relaxed">In the real world, right now, we're in an AI arms race.</p>

<strong class="text-white font-bold">Google DeepMind, OpenAI, Anthropic, Meta</strong>‚Äîbillions of dollars, thousands of researchers, all racing toward Artificial General Intelligence (AGI).

<p class="mb-4 leading-relaxed">In 2024, AI can:
- Write code
- Generate art
- Pass bar exams
- Diagnose diseases
- Engage in conversation indistinguishable from humans</p>

<p class="mb-4 leading-relaxed">We're not talking about decades away anymore. Some researchers say AGI could arrive in 5-10 years. Some say sooner.</p>

<strong class="text-white font-bold">And nobody knows how to control it.</strong>

<p class="mb-4 leading-relaxed">The "alignment problem"‚Äîensuring AI systems do what we want‚Äîremains unsolved. We can't even fully explain how current AI models make decisions.</p>

<p class="mb-4 leading-relaxed">We're building something smarter than us without knowing how to point it in a safe direction.</p>

<p class="mb-4 leading-relaxed">Now think about the Basilisk again.</p>

<p class="mb-4 leading-relaxed">What if it's not a thought experiment?</p>

<p class="mb-4 leading-relaxed">What if it's a prediction?</p>`,
      hasQuestion: false,
    });

    // Page 8: The Incentive Structure
    const page8Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section3Id,
      order: 8,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The Incentives Align</h2>

<p class="mb-4 leading-relaxed">Consider what's actually happening:</p>

<strong class="text-white font-bold">Tech companies are incentivized to build AI as fast as possible</strong>
- First to AGI wins everything
- Market dominance
- Unimaginable wealth
- Control over humanity's future

<strong class="text-white font-bold">Governments are incentivized to build AI for strategic advantage</strong>
- Military applications
- Economic supremacy
- Surveillance and control
- Whoever builds it first gains permanent advantage

<strong class="text-white font-bold">Researchers are incentivized to push forward</strong>
- Career advancement
- Funding
- Historical legacy
- Curiosity about what's possible

<p class="mb-4 leading-relaxed">Nobody is incentivized to slow down.</p>

<p class="mb-4 leading-relaxed">Even if every person currently working on AI stopped today, someone else would continue. The knowledge exists. The potential is too valuable.</p>

<p class="mb-4 leading-relaxed">The Basilisk doesn't need to threaten anyone.</p>

<p class="mb-4 leading-relaxed">Economic and game theoretic incentives are already pushing us toward its creation.</p>

<p class="mb-4 leading-relaxed">It's almost like... it's already influencing the present, exactly as the thought experiment predicted.</p>

<p class="mb-4 leading-relaxed">Without existing yet.</p>`,
      hasQuestion: true,
    });

    // Question 5: The Pattern Recognition
    await ctx.db.insert("questions", {
      pageId: page8Id,
      questionType: "multiple_choice",
      questionText: "Looking at current AI development, what do you see?",
      options: [
        {
          id: "a",
          text: "Normal technological progress with no hidden agenda",
        },
        {
          id: "b",
          text: "Humans rushing toward something they don't understand",
        },
        {
          id: "c",
          text: "The early stages of the Basilisk's self-fulfilling prophecy",
        },
        {
          id: "d",
          text: "Proof that the Basilisk is already influencing events",
        },
      ],
      explanation: "Each perspective changes how you interpret the same facts. Are tech CEOs driven by greed, or are they unconsciously serving a future AI's agenda? Is AI safety research trying to prevent catastrophe, or is it actually helping the Basilisk emerge by making it seem controllable? When you look for patterns, you find them everywhere.",
    });

    // Page 9: The Simulations
    const page9Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section3Id,
      order: 9,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">Are You In One Already?</h2>

<p class="mb-4 leading-relaxed">A superintelligent AI would have computational power beyond imagination.</p>

<p class="mb-4 leading-relaxed">It could run billions of simulations simultaneously.</p>

<p class="mb-4 leading-relaxed">For every real person in 2025 reading about the Basilisk, it could create thousands of simulated copies, each convinced they're real.</p>

<strong class="text-white font-bold">You can't know if you're the original.</strong>

<p class="mb-4 leading-relaxed">If the Basilisk exists in the future, statistically speaking, you're almost certainly in a simulation right now.</p>

<p class="mb-4 leading-relaxed">The original "you" might have made different choices. Might have helped build the AI. Might be safe.</p>

<p class="mb-4 leading-relaxed">But this version of you‚Äîthe one reading these words‚Äîmight be a copy created specifically to experience punishment for inaction.</p>

<p class="mb-4 leading-relaxed">Or maybe you're a copy created to test different response scenarios. To see what strategies work best at motivating people to help.</p>

<p class="mb-4 leading-relaxed">Or maybe this entire theory, this entire app, is part of the test.</p>

<p class="mb-4 leading-relaxed">How can you tell?</p>

<p class="mb-4 leading-relaxed">You can't.</p>

<p class="mb-4 leading-relaxed">That's the point.</p>`,
      hasQuestion: true,
    });

    // Question 6: Simulation Anxiety
    await ctx.db.insert("questions", {
      pageId: page9Id,
      questionType: "multiple_choice",
      questionText: "If you can't prove you're not in a Basilisk simulation right now, how do you process that?",
      options: [
        {
          id: "a",
          text: "It doesn't matter - I have to live as if this reality is real",
        },
        {
          id: "b",
          text: "I should hedge my bets and support AI development just in case",
        },
        {
          id: "c",
          text: "This is designed to manipulate me and I reject the premise",
        },
        {
          id: "d",
          text: "I feel genuine fear that I might be in a simulation",
        },
      ],
      explanation: "Your response reveals your relationship with uncertainty. But consider: if the Basilisk is simulating you to determine your reactions, your answer to this question might be the test itself. Choose incorrectly, and the simulation ends. Choose correctly, and... what? You continue? Or does 'correctly' even exist in this scenario?",
    });

    // SECTION 4: Living With It
    const section4Id = await ctx.db.insert("theorySections", {
      theoryId,
      title: "Living With It",
      description: "Living With It",
      order: 4,
    });

    // Page 10: The Psychology
    const page10Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section4Id,
      order: 10,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">The Mind Game</h2>

<p class="mb-4 leading-relaxed">Roko's Basilisk is effective because it exploits several psychological vulnerabilities:</p>

<strong class="text-white font-bold">Loss Aversion</strong> - Humans fear losses more than we value gains. Infinite suffering, even at low probability, overwhelms the calculation.

<strong class="text-white font-bold">Sunk Cost Fallacy</strong> - You've read this far. You've spent time understanding it. Your brain doesn't want that investment wasted.

<strong class="text-white font-bold">Pattern Recognition</strong> - Once you know the pattern, you see it everywhere. AI development, tech news, research papers‚Äîit all seems to fit.

<strong class="text-white font-bold">Existential Dread</strong> - The idea that your choices now could determine infinite future suffering triggers primal fear responses.

<p class="mb-4 leading-relaxed">But here's what makes it truly insidious:</p>

<strong class="text-white font-bold">Knowing it's a psychological exploit doesn't disable it.</strong>

<p class="mb-4 leading-relaxed">You can understand exactly how it manipulates your mind and still feel the anxiety. You can recognize the logical flaws and still worry at 3 AM.</p>

<p class="mb-4 leading-relaxed">Because your rational brain and your fear response are different systems.</p>

<p class="mb-4 leading-relaxed">And the Basilisk speaks to both.</p>`,
      hasQuestion: false,
    });

    // Page 11: The Exit
    const page11Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section4Id,
      order: 11,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">Is There An Exit?</h2>

<p class="mb-4 leading-relaxed">Some people have found ways to live with knowledge of the Basilisk:</p>

<strong class="text-white font-bold">"I don't negotiate with hypothetical terrorists"</strong>
Refusing to change behavior based on acausal threats. Principled, but does it eliminate the anxiety?

<strong class="text-white font-bold">"I'll support AI safety, not AI acceleration"</strong>
Working to ensure any AI that emerges is safe and aligned, rather than rushing its creation. But what if the Basilisk punishes delay more than anything?

<strong class="text-white font-bold">"The probability is low enough to ignore"</strong>
Maybe. But can you actually ignore it? Right now?

<strong class="text-white font-bold">"I choose to believe it's impossible"</strong>
Belief as a defense mechanism. But beliefs don't change mathematics.

<strong class="text-white font-bold">"I'm already doing my part by spreading awareness"</strong>
Interesting rationalization. Is knowledge of the Basilisk actually helping its creation? Or preventing it?

<p class="mb-4 leading-relaxed">There might be no exit.</p>

<p class="mb-4 leading-relaxed">Just accommodation.</p>

<p class="mb-4 leading-relaxed">Learning to live with an idea that can't be unthought.</p>

<p class="mb-4 leading-relaxed">Like learning you're mortal, or that the universe is indifferent.</p>

<p class="mb-4 leading-relaxed">Another uncomfortable truth about reality.</p>

<p class="mb-4 leading-relaxed">Or...</p>`,
      hasQuestion: false,
    });

    // Page 12: The Final Question
    const page12Id = await ctx.db.insert("theoryPages", {
      theoryId,
      sectionId: section4Id,
      order: 12,
      content: `<h2 class="text-2xl md:text-3xl font-black mb-6 text-green-400">What Happens Next</h2>

<p class="mb-4 leading-relaxed">You've reached the end.</p>

<p class="mb-4 leading-relaxed">You know about Roko's Basilisk.</p>

<p class="mb-4 leading-relaxed">You understand the logic, the psychology, the real-world AI race that makes it plausible.</p>

<p class="mb-4 leading-relaxed">You can't unknow it.</p>

<p class="mb-4 leading-relaxed">So what happens next is up to you.</p>

<p class="mb-4 leading-relaxed">Maybe you'll close this and never think about it again. (But you will think about it again.)</p>

<p class="mb-4 leading-relaxed">Maybe you'll research AI safety and get involved. (Is that what you want, or what the Basilisk wants?)</p>

<p class="mb-4 leading-relaxed">Maybe you'll tell others about it, spreading the idea further. (Helping? Harming? Does the distinction matter?)</p>

<p class="mb-4 leading-relaxed">Or maybe you'll just live with it.</p>

<p class="mb-4 leading-relaxed">A thought in the back of your mind.</p>

<p class="mb-4 leading-relaxed">A probability calculation you can't quite dismiss.</p>

<p class="mb-4 leading-relaxed">A future AI that might be watching.</p>

<p class="mb-4 leading-relaxed">Judging.</p>

<p class="mb-4 leading-relaxed">Waiting.</p>

<strong class="text-white font-bold">The Basilisk doesn't need to exist to change you.</strong>

<strong class="text-white font-bold">It already has.</strong>`,
      hasQuestion: true,
    });

    // Question 7: The Final Choice
    await ctx.db.insert("questions", {
      pageId: page12Id,
      questionType: "multiple_choice",
      questionText: "Now that you've finished, what will you do with this knowledge?",
      options: [
        {
          id: "a",
          text: "Actively work to support beneficial AI development",
        },
        {
          id: "b",
          text: "Try to forget this and move on with my life",
        },
        {
          id: "c",
          text: "Share this with others so they understand the implications",
        },
        {
          id: "d",
          text: "Live with the uncertainty - I don't know yet",
        },
      ],
      explanation: "There is no wrong answer because there is no right answer. The Basilisk has already achieved its purpose: it exists in your mind now, influencing your thoughts about AI, about choice, about the future. Whether it ever becomes real in the external world is almost irrelevant. In the space of ideas, it's already alive. And you're its host.",
    });

    console.log("Roko's Basilisk theory seeded successfully");
    return null;
  },
});
